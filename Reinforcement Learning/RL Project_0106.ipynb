{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<font size=\"8\">Reinforcement Learning: Project - Highway-Environment</font>\n",
    "\n",
    "Group: `el_grupo_87`\n",
    "\n",
    "Group members:\n",
    "1. `André Moreira Lopes : 20230570`\n",
    "2. `Luís Queiroz : 20230584`\n",
    "3. `André Filipe Silva : 20230972`\n",
    "4. `Pedro Cerejeira : 20230442`\n",
    "5. `João Gonçalves : 20230560`"
   ],
   "id": "a3738691e65de669"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c408d461",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Final Project \n",
    "\n",
    "Welcome to your Reinforcement Learning project! Join in groups of a maximum of 5 students on a project focused on developing an RL agent capable of solving an environment for decision-making in Autonomous Driving. The project deadline has been set to the 2nd of June.\n",
    "\n",
    "Autonomous Driving has long been considered a field in which RL algorithms excel, and this project aims to leverage the power of RL to create an intelligent agent that can solve the Farama’s foundation “highway-env” project, namely the Highway environment (refer to https://highway-env.farama.org/environments/highway/).\n",
    "\n",
    "## Project Requirements:\n",
    "\n",
    "* The environments observation’s format can vary according to our preference, namely Kinematics, Grayscale Image, Occupancy grid and Time to collision (refer to https://highway-env.farama.org/observations/). In your solutions you should use 2 of these types.\n",
    "* The agents actions can also vary, as continuous actions, discrete actions and discrete meta-actions (refer to https://highway-env.farama.org/actions/). In your solutions you should use 2 of these types.\n",
    "* As for the algorithms to use, any algorithm is valid (seen or not in class), with a minimum requirement of 3 different algorithms used.\n",
    "* Apart from the environment observation types and agent action types you must use environment’s configuration provided in the annexed notebook!\n",
    "Note: Your delivery should comprise 4 solutions to the highway environment (corresponding to the combinations of the two environment observation’s types and the two agent’s action types), in which you just need to use one algorithm for each combination (knowing that you need to use at least 3 different algorithms).\n",
    "\n",
    "\n",
    "## Project Objectives:\n",
    "\n",
    "* Train an RL agent to solve the Highway environment: The primary objective of this project is to develop an RL agent that can maximize the reward given by the highway environment (refer to https://highway-env.farama.org/rewards/), which leverages to maximize speed while minimizing crash risk! \n",
    "* Optimize decision-making using RL algorithms: Explore different RL algorithms to train the agent. Compare and analyse their effectiveness in learning and decision-making capabilities in the context of the environment.\n",
    "* Explore and expand on the reward system: Although you should evaluate your agent with the reward function provided by the environment, you could/should expand it to better train your agent.\n",
    "* Enhance interpretability and analysis: Develop methods to analyse the agent's decision-making process and provide insights into its strategic thinking. Investigate techniques to visualize the agent's evaluation of chess positions and understand its reasoning behind specific moves.\n",
    "\n",
    "\n",
    "\n",
    "### Extra Objectives:\n",
    "\n",
    "* Investigate transfer learning and generalization: Explore techniques for transfer learning to leverage knowledge acquired in related domains or from pre-training on large chess datasets. Investigate the agent's ability to generalize its knowledge.\n",
    "* Explore multi agent approaches: The environment allows you to use more than one agent per episode. Explore multi agent alternatives to improve your learning times and overall benchmarks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78aabeb2",
   "metadata": {},
   "source": [
    "## Imports Required\n",
    "\n",
    "You might need to restart the kernel after installation"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0b431af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.524002Z",
     "start_time": "2024-06-11T13:14:37.518819Z"
    }
   },
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "from typing import List, Dict, Optional, Any\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.745547Z",
     "start_time": "2024-06-11T13:14:37.740531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Num GPUs Available: ', len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "print('GPU Available:', torch.cuda.is_available())"
   ],
   "id": "1c3db61ede476dc9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "GPU Available: True\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "9f15ebae",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "Apart from the environment observation types and agent action types you must use some of the environment’s configurations provided bellow!"
   ]
  },
  {
   "cell_type": "code",
   "id": "ee4117ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.764175Z",
     "start_time": "2024-06-11T13:14:37.757146Z"
    }
   },
   "source": [
    "def config_generator(\n",
    "        obs_action_type: Optional[Dict[str, Dict[str, str]]] = None, \n",
    "        reward_params: Optional[Dict[str, Any]] = None, \n",
    "        policy_frequency: Optional[Dict[str, int]] = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generate a configuration dictionary for a highway environment with customizable observation, action, and reward parameters.\n",
    "\n",
    "    Args:\n",
    "        obs_action_type (dict, optional): Dictionary specifying observation and action types.\n",
    "            Default is None, which uses:\n",
    "            {\n",
    "                \"observation\": {\n",
    "                    \"type\": \"Kinematics\"\n",
    "                },\n",
    "                \"action\": {\n",
    "                    \"type\": \"DiscreteMetaAction\"\n",
    "                }\n",
    "            }\n",
    "        reward_params (dict, optional): Dictionary specifying reward-related parameters.\n",
    "            Default is None, which uses:\n",
    "            {\n",
    "                'collision_reward': -1,\n",
    "                'reward_speed_range': [20, 30],\n",
    "                'simulation_frequency': 15,\n",
    "            }\n",
    "        policy_frequency (dict, optional): Dictionary specifying the policy frequency parameter.\n",
    "            Default is None, which uses:\n",
    "            {\n",
    "                'policy_frequency': 1\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the configured parameters for the highway environment.\n",
    "        \n",
    "    Configuration Parameters:\n",
    "    - lanes_count (int): Number of lanes in the highway (fixed to 10).\n",
    "    - vehicles_count (int): Number of other vehicles in the environment (fixed to 50).\n",
    "    - duration (int): Duration of the environment in seconds (fixed to 120 seconds).\n",
    "    - other_vehicles_type (str): Policy type for other vehicles in the environment.\n",
    "    - initial_spacing (int): Initial spacing between vehicles.\n",
    "    - screen_width (int): Width of the visualization screen in pixels.\n",
    "    - screen_height (int): Height of the visualization screen in pixels.\n",
    "    - centering_position (list): Centering position of the visualization.\n",
    "    - scaling (int): Scaling factor for visualization.\n",
    "    - show_trajectories (bool): Flag to show trajectories in visualization.\n",
    "    - render_agent (bool): Flag to render the agent in visualization.\n",
    "    - offscreen_rendering (bool): Flag to enable offscreen rendering.\n",
    "    - offroad_terminal (bool): Flag to enable terminal state when the ego vehicle goes off the road.\n",
    "\n",
    "    See Also:\n",
    "        - Observation space types: https://highway-env.readthedocs.io/en/latest/user_guide/observation_spaces.html\n",
    "        - Action space types: https://highway-env.readthedocs.io/en/latest/user_guide/action_spaces.html\n",
    "    \"\"\"\n",
    "    # Default observation and action types\n",
    "    if obs_action_type is None:\n",
    "        obs_action_type = {\n",
    "            'observation': {\n",
    "                'type': 'Kinematics'\n",
    "            },\n",
    "            'action': {\n",
    "                'type': 'DiscreteMetaAction'\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # Default reward-related parameters\n",
    "    if reward_params is None:\n",
    "        reward_params = {\n",
    "            'collision_reward': -1,\n",
    "            'reward_speed_range': [20, 30],\n",
    "            'simulation_frequency': 15,\n",
    "        }\n",
    "    \n",
    "    # Default policy frequency parameter\n",
    "    if policy_frequency is None:\n",
    "        policy_frequency = {'policy_frequency': 1}\n",
    "\n",
    "    # Base configuration parameters\n",
    "    configuration = {\n",
    "        # Parameters below cannot be changed\n",
    "        'lanes_count': 10,  # The environment must always have 10 lanes\n",
    "        'vehicles_count': 50,  # The environment must always have 50 other vehicles\n",
    "        'duration': 120,  # [s] The environment must terminate never before 120 seconds\n",
    "        'other_vehicles_type': 'highway_env.vehicle.behavior.IDMVehicle',  # This is the policy of the other vehicles\n",
    "        'initial_spacing': 2,  # Initial spacing between vehicles needs to be at most 2\n",
    "\n",
    "        # Refer to https://highway-env.farama.org/observations/ to change observation space type\n",
    "        'observation': {\n",
    "            'type': 'Kinematics'\n",
    "        },\n",
    "\n",
    "        # Refer to https://highway-env.farama.org/actions/ to change action space type\n",
    "        'action': {\n",
    "            'type': 'DiscreteMetaAction',\n",
    "        },\n",
    "\n",
    "        # Parameters below can be changed (as it refers mostly to the reward system)\n",
    "        'collision_reward': -1,  # The reward received when colliding with a vehicle. (Can be changed)\n",
    "        'reward_speed_range': [20, 30],  # [m/s] The reward for high speed is mapped linearly from this range to [0, HighwayEnv.HIGH_SPEED_REWARD]. (Can be changed)\n",
    "        'simulation_frequency': 15,  # [Hz] (Can be changed)\n",
    "        'policy_frequency': 1,  # [Hz] (Can be changed)\n",
    "        \n",
    "        # Parameters defined below are purely for visualization purposes! You can alter them as you please\n",
    "        'screen_width': 800,  # [px]\n",
    "        'screen_height': 600,  # [px]\n",
    "        'centering_position': [0.5, 0.5],\n",
    "        'scaling': 5,\n",
    "        'show_trajectories': False,\n",
    "        'render_agent': True,\n",
    "        'offscreen_rendering': False,\n",
    "        \n",
    "        # Auxiliary Parameters\n",
    "        'slower_than_others_penalty': False,\n",
    "        'lane_centering_cost': 1\n",
    "    }\n",
    "\n",
    "    # Update configuration with observation and action parameters\n",
    "    configuration.update(obs_action_type)\n",
    "    \n",
    "    # Update configuration with reward parameters\n",
    "    configuration.update(reward_params)\n",
    "    \n",
    "    # Update configuration with policy frequency\n",
    "    configuration.update(policy_frequency)\n",
    "\n",
    "    return configuration"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.795429Z",
     "start_time": "2024-06-11T13:14:37.790862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_env(id: str = 'highway-v0', render_mode: Optional[str] = None, config: Optional[Dict[str, Any]] = None) -> gym.Env:\n",
    "    \"\"\"\n",
    "    Create a gym environment with the given ID, render mode, and configuration.\n",
    "\n",
    "    Args:\n",
    "        id (str): The ID of the gym environment to create. Default is 'highway-v0'.\n",
    "        render_mode (str, optional): The render mode for the environment. Default is None.\n",
    "        config (dict, optional): Configuration dictionary for the environment. Default is None, \n",
    "                                 which uses the default configuration from config_generator().\n",
    "\n",
    "    Returns:\n",
    "        gym.Env: The created gym environment.\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = config_generator()\n",
    "    return gym.make(id=id, render_mode=render_mode, config=config)"
   ],
   "id": "8ba321b76e6a7b6c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.844775Z",
     "start_time": "2024-06-11T13:14:37.840771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def record_step_data(step_rewards: List[float], actions: List, policy_frequency: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Record step/action data into a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        step_rewards (List[float]): List of rewards for each step/action.\n",
    "        actions (List): List of actions taken.\n",
    "        policy_frequency (int): Policy frequency for recording actions.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the rewards and actions for each step.\n",
    "    \"\"\"\n",
    "    step_data = {\n",
    "        'reward': [],\n",
    "        'action': []\n",
    "    }\n",
    "\n",
    "    # Process each set of steps based on the policy frequency\n",
    "    for i in range(0, len(step_rewards), policy_frequency):\n",
    "        # Average the rewards for the current set of steps\n",
    "        avg_reward = np.mean(step_rewards[i:i + policy_frequency])\n",
    "        # List of actions taken in the current set of steps\n",
    "        action_list = actions[i:i + policy_frequency]\n",
    "        \n",
    "        step_data['reward'].append(avg_reward)\n",
    "        step_data['action'].append(action_list)\n",
    "    \n",
    "    return pd.DataFrame(step_data)"
   ],
   "id": "7023fa36125e5319",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.893853Z",
     "start_time": "2024-06-11T13:14:37.886266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, environment, model_name, max_episodes=10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate the given RL model in the specified environment and output a .xlsx file with the results.\n",
    "\n",
    "    Args:\n",
    "        model (BaseAlgorithm): The RL model to be evaluated.\n",
    "        environment (gym.environment): The environment in which to evaluate the model.\n",
    "        model_name (str): The name of the model passed.\n",
    "        max_episodes (int): Number of episodes to evaluate the model on. \n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the evaluation results per episode.\n",
    "    \"\"\"\n",
    "    policy_frequency = environment.config['policy_frequency']\n",
    "    output_filename = f'results/{environment.config[\"action\"][\"type\"]}_{environment.config[\"observation\"][\"type\"]}_{model_name}.xlsx'\n",
    "\n",
    "    obs = environment.reset()\n",
    "    episodes_data = []\n",
    "    step_data_frames = []\n",
    "    \n",
    "    for episode in range(max_episodes):\n",
    "        start_time = time.time()\n",
    "        episode_returns = 0.0\n",
    "        step_rewards = []\n",
    "        actions = []\n",
    "\n",
    "        while True:\n",
    "            if isinstance(obs, tuple):\n",
    "                obs = obs[0]\n",
    "\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, truncated, _ = environment.step(action)\n",
    "            \n",
    "            step_rewards.append(reward)\n",
    "            actions.append(action)\n",
    "            episode_returns += reward\n",
    "\n",
    "            if done or truncated:\n",
    "                end_time = time.time()\n",
    "                episode_duration = end_time - start_time\n",
    "                normalized_return = episode_returns / policy_frequency\n",
    "                \n",
    "                episodes_data.append({\n",
    "                    'average_standardized_reward': normalized_return,\n",
    "                    'episode_duration': episode_duration,\n",
    "                    'returns_list': (normalized_return, truncated)\n",
    "                })\n",
    "                \n",
    "                # Record step data\n",
    "                step_data_df = record_step_data(step_rewards, actions, policy_frequency)\n",
    "                step_data_frames.append((f'episode_{episode + 1}_steps', step_data_df))\n",
    "                \n",
    "                obs = environment.reset()\n",
    "                break\n",
    "\n",
    "    # Convert episodes data to DataFrame\n",
    "    episodes_df = pd.DataFrame(episodes_data)\n",
    "\n",
    "    # Write to Excel with multiple sheets\n",
    "    with pd.ExcelWriter(output_filename) as writer:\n",
    "        episodes_df.to_excel(writer, sheet_name='overall_evaluation', index=False)\n",
    "        for sheet_name, df in step_data_frames:\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    return episodes_df"
   ],
   "id": "c2d0ffebc337305",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Registering Custom Enviroment",
   "id": "6830dcc5a65d57c7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:37.947838Z",
     "start_time": "2024-06-11T13:14:37.942746Z"
    }
   },
   "cell_type": "code",
   "source": "gym.envs.register(id='CustomHighwayEnv-v0', entry_point='highway_env_custom:CustomHighwayEnv')",
   "id": "2c6a30a6a350f424",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\envs\\registration.py:693: UserWarning: \u001B[33mWARN: Overriding environment CustomHighwayEnv-v0 already in registry.\u001B[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Proving it is just an extension of the base environment",
   "id": "63201e0beddb34bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.016666Z",
     "start_time": "2024-06-11T13:14:37.981067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "highway = create_env(id='highway-v0', render_mode=None , config=None)\n",
    "custom_highway = create_env(id='CustomHighwayEnv-v0', render_mode=None, config=None)"
   ],
   "id": "7b9a1526984b239c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001B[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.030756Z",
     "start_time": "2024-06-11T13:14:38.027192Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f'Highway Observation Space: {highway.observation_space.shape} \\n Highway Action Space:{highway.action_space.shape} \\n Type: {type(highway)}')\n",
    "print('')\n",
    "print(f'Custom Highway Observation Space: {custom_highway.observation_space.shape} \\n Custom Highway Action Space:{custom_highway.action_space.shape} \\n Type:{type(custom_highway)}')"
   ],
   "id": "526de62db8f9c03c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highway Observation Space: (5, 5) \n",
      " Highway Action Space:() \n",
      " Type: <class 'gymnasium.wrappers.order_enforcing.OrderEnforcing'>\n",
      "\n",
      "Custom Highway Observation Space: (5, 5) \n",
      " Custom Highway Action Space:() \n",
      " Type:<class 'gymnasium.wrappers.order_enforcing.OrderEnforcing'>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "c52431cb",
   "metadata": {},
   "source": [
    "## Solution 0 - Example Solution\n",
    "\n",
    "\n",
    "Environment Observation Type: **Kinematics** \\\n",
    "Agent Action Type: **DiscreteMetaAction** \\\n",
    "Algorithm Used: **Random**\n",
    "\n",
    "Example of the environment's usage using a random policy."
   ]
  },
  {
   "cell_type": "code",
   "id": "196cb827",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.067744Z",
     "start_time": "2024-06-11T13:14:38.064743Z"
    }
   },
   "source": [
    "# env = gym.make('highway-v0', render_mode='human', config=configuration)\n",
    "# \n",
    "# obs, info = env.reset(seed=42)\n",
    "# done = truncated = False\n",
    "# \n",
    "# Return = 0\n",
    "# n_steps = 1\n",
    "# Episode = 0\n",
    "# while not (done or truncated):\n",
    "#   # Dispatch the observations to the model to get the tuple of actions\n",
    "#   action = env.action_space.sample()\n",
    "#   # Execute the actions\n",
    "#   next_obs, reward, done, truncated, info = env.step(action)\n",
    "#   Return += reward\n",
    "# \n",
    "#   print('Episode: {}, Step: {}, Return: {}'.format(Episode, n_steps, round(Return,2)))\n",
    "#   n_steps+=1\n",
    "# env.close()"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "b653a88d",
   "metadata": {},
   "source": [
    "## Solution 1 - DiscreteMeta_Kinematics\n",
    "Environment Observation Type: **Kinematics** \\\n",
    "Agent Action Type: **DiscreteMetaAction** \\\n",
    "Algorithm Used:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configs",
   "id": "5ccf4fff5b38684"
  },
  {
   "cell_type": "code",
   "id": "633489a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.126095Z",
     "start_time": "2024-06-11T13:14:38.122585Z"
    }
   },
   "source": [
    "discrete_meta_kinematics_1 = config_generator()\n",
    "discrete_meta_kinematics_2 = config_generator(reward_params={'collision_reward': -4})"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 1",
   "id": "f369522cfe600661"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (training)",
   "id": "cad977883b4861b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.169021Z",
     "start_time": "2024-06-11T13:14:38.166020Z"
    }
   },
   "cell_type": "code",
   "source": "# env_dmk = create_env(id='highway-v0', render_mode=None, config=discrete_meta_kinematics_1)",
   "id": "1a918e8f89304a1e",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.207864Z",
     "start_time": "2024-06-11T13:14:38.205340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = DQN('MlpPolicy',\n",
    "#             env_dmk,\n",
    "#             learning_rate=5e-4,\n",
    "#             buffer_size=500000,\n",
    "#             learning_starts=200,\n",
    "#             batch_size=32,\n",
    "#             gamma=0.8,\n",
    "#             train_freq=1,\n",
    "#             gradient_steps=1,\n",
    "#             target_update_interval=50,\n",
    "#             exploration_fraction=0.7,\n",
    "#             verbose=1,\n",
    "#             tensorboard_log='highway_dqn/')\n",
    "# \n",
    "# model.learn(total_timesteps=int(20000))\n",
    "# \n",
    "# model.save('models/DiscreteMeta_Kinematics/dqn_1')"
   ],
   "id": "bb394c3ec5452458",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (evaluation)",
   "id": "83fbbef918b0338c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.264568Z",
     "start_time": "2024-06-11T13:14:38.237959Z"
    }
   },
   "cell_type": "code",
   "source": "dqn_1_dmk = DQN.load('models/DiscreteMeta_Kinematics/dqn_1')",
   "id": "17cd10a92cd479f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: Can't get attribute '_function_setstate' on <module 'cloudpickle.cloudpickle' from 'C:\\\\Users\\\\pedro\\\\anaconda3\\\\envs\\\\py310\\\\lib\\\\site-packages\\\\cloudpickle\\\\cloudpickle.py'>\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: Can't get attribute '_function_setstate' on <module 'cloudpickle.cloudpickle' from 'C:\\\\Users\\\\pedro\\\\anaconda3\\\\envs\\\\py310\\\\lib\\\\site-packages\\\\cloudpickle\\\\cloudpickle.py'>\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:14:38.297550Z",
     "start_time": "2024-06-11T13:14:38.276295Z"
    }
   },
   "cell_type": "code",
   "source": "env_dmk_1 = create_env(id='highway-v0', render_mode='human', config=config_generator())",
   "id": "698a6841e9bcdf36",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:17:04.339902Z",
     "start_time": "2024-06-11T13:14:38.298829Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmk_1 = evaluate_model(dqn_1_dmk, env_dmk_1, 'DQN_1')",
   "id": "68d70c47518d3322",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:17:04.350424Z",
     "start_time": "2024-06-11T13:17:04.340916Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmk_1",
   "id": "d3a41eda3e505ffe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                 returns_list\n",
       "0                    28.640696         12.721871  (28.640696452919983, False)\n",
       "1                    23.530443          9.676084   (23.53044325085273, False)\n",
       "2                     3.993524          1.837489  (3.9935242319006594, False)\n",
       "3                    93.313199         39.018213    (93.31319859856248, True)\n",
       "4                     2.780063          1.314662   (2.780063215891057, False)\n",
       "5                    37.136821         13.277817   (37.13682120528132, False)\n",
       "6                    93.820565         38.929678    (93.82056474095737, True)\n",
       "7                    34.897683         13.410062   (34.89768278627791, False)\n",
       "8                    18.914938          7.633412   (18.91493828384177, False)\n",
       "9                    17.489109          7.945916  (17.489109097002785, False)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.640696</td>\n",
       "      <td>12.721871</td>\n",
       "      <td>(28.640696452919983, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.530443</td>\n",
       "      <td>9.676084</td>\n",
       "      <td>(23.53044325085273, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.993524</td>\n",
       "      <td>1.837489</td>\n",
       "      <td>(3.9935242319006594, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93.313199</td>\n",
       "      <td>39.018213</td>\n",
       "      <td>(93.31319859856248, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.780063</td>\n",
       "      <td>1.314662</td>\n",
       "      <td>(2.780063215891057, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37.136821</td>\n",
       "      <td>13.277817</td>\n",
       "      <td>(37.13682120528132, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93.820565</td>\n",
       "      <td>38.929678</td>\n",
       "      <td>(93.82056474095737, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34.897683</td>\n",
       "      <td>13.410062</td>\n",
       "      <td>(34.89768278627791, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>18.914938</td>\n",
       "      <td>7.633412</td>\n",
       "      <td>(18.91493828384177, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17.489109</td>\n",
       "      <td>7.945916</td>\n",
       "      <td>(17.489109097002785, False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 2",
   "id": "e027800937c1c07c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (training)",
   "id": "bd79c21b5e83fe1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:17:04.354937Z",
     "start_time": "2024-06-11T13:17:04.350928Z"
    }
   },
   "cell_type": "code",
   "source": "# env_dmk = create_env(id='highway-v0', render_mode=None, config=discrete_meta_kinematics_2)",
   "id": "7119252b9d68fb61",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:17:04.360440Z",
     "start_time": "2024-06-11T13:17:04.355937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = DQN('MlpPolicy',\n",
    "#             env_dmk,\n",
    "#             learning_rate=5e-4,\n",
    "#             buffer_size=500000,\n",
    "#             learning_starts=200,\n",
    "#             batch_size=32,\n",
    "#             gamma=0.8,\n",
    "#             train_freq=1,\n",
    "#             gradient_steps=1,\n",
    "#             target_update_interval=50,\n",
    "#             exploration_fraction=0.7,\n",
    "#             verbose=1,\n",
    "#             tensorboard_log='highway_dqn/')\n",
    "# \n",
    "# model.learn(total_timesteps=int(20000))\n",
    "# \n",
    "# model.save('models/DiscreteMeta_Kinematics/dqn_2')"
   ],
   "id": "2e703839f7571b5e",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (evaluation)",
   "id": "48b81aa3afba7efb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:17:04.388062Z",
     "start_time": "2024-06-11T13:17:04.362460Z"
    }
   },
   "cell_type": "code",
   "source": "dqn_2_dmk = DQN.load('models/DiscreteMeta_Kinematics/dqn_2')",
   "id": "815a782f5bd08cd9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:17:04.408264Z",
     "start_time": "2024-06-11T13:17:04.388062Z"
    }
   },
   "cell_type": "code",
   "source": "env_dmk_1 = create_env(id='highway-v0', render_mode='human', config=config_generator())",
   "id": "5c7ff12a71e3c80b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 5)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.651882Z",
     "start_time": "2024-06-11T13:17:04.408264Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmk_2 = evaluate_model(dqn_2_dmk, env_dmk_1, 'DQN_2')",
   "id": "b7caccafc47711f1",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.669785Z",
     "start_time": "2024-06-11T13:21:16.653413Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmk_2",
   "id": "891aa0ea1bce74a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                 returns_list\n",
       "0                    85.002782         38.857615    (85.00278162863779, True)\n",
       "1                    86.427310         39.010568    (86.42730958013301, True)\n",
       "2                     6.313688          2.910963   (6.313687647808391, False)\n",
       "3                    20.187708          8.968179  (20.187708382951605, False)\n",
       "4                    54.121291         23.078587   (54.12129074028867, False)\n",
       "5                     1.604160          0.973331  (1.6041598054128516, False)\n",
       "6                    82.867849         38.165244    (82.86784876869774, True)\n",
       "7                    91.847244         38.116300    (91.84724401045355, True)\n",
       "8                    48.788388         21.970414  (48.788387544542644, False)\n",
       "9                    85.907504         39.841136    (85.90750365522582, True)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.002782</td>\n",
       "      <td>38.857615</td>\n",
       "      <td>(85.00278162863779, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.427310</td>\n",
       "      <td>39.010568</td>\n",
       "      <td>(86.42730958013301, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.313688</td>\n",
       "      <td>2.910963</td>\n",
       "      <td>(6.313687647808391, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.187708</td>\n",
       "      <td>8.968179</td>\n",
       "      <td>(20.187708382951605, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54.121291</td>\n",
       "      <td>23.078587</td>\n",
       "      <td>(54.12129074028867, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.604160</td>\n",
       "      <td>0.973331</td>\n",
       "      <td>(1.6041598054128516, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82.867849</td>\n",
       "      <td>38.165244</td>\n",
       "      <td>(82.86784876869774, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>91.847244</td>\n",
       "      <td>38.116300</td>\n",
       "      <td>(91.84724401045355, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48.788388</td>\n",
       "      <td>21.970414</td>\n",
       "      <td>(48.788387544542644, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85.907504</td>\n",
       "      <td>39.841136</td>\n",
       "      <td>(85.90750365522582, True)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "id": "a1653ba8",
   "metadata": {},
   "source": [
    "## Solution 2 - Continuous_Kinematics\n",
    "Environment Observation Type: **Kinematics** \\\n",
    "Agent Action Type: **Continuous Actions** \\\n",
    "Algorithm Used: **PPO**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Config",
   "id": "c47ea2603c2aaf5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.680894Z",
     "start_time": "2024-06-11T13:21:16.671249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "continuous_kinematics = config_generator(\n",
    "    obs_action_type={\n",
    "        'observation': {\n",
    "            'type': 'Kinematics',\n",
    "            'vehicles_count': 5,\n",
    "            'features': [\n",
    "                'presence',\n",
    "                'x',\n",
    "                'y',\n",
    "                'vx',\n",
    "                'vy',\n",
    "                'cos_h',\n",
    "                'sin_h'\n",
    "            ],\n",
    "            'absolute': False\n",
    "        },\n",
    "        'action': {\n",
    "            'type': 'ContinuousAction',\n",
    "        },     \n",
    "    },\n",
    "    reward_params={\n",
    "        'collision_reward': -5, \n",
    "        'reward_speed_range': [10, 40],\n",
    "        'simulation_frequency': 15,\n",
    "        'speed_reward': 5, \n",
    "        'lane_change_penalty': -0.2,\n",
    "        'offroad_terminal': True,\n",
    "        'lane_centering_cost': 2,\n",
    "        'lane_centering_reward': 0.2,\n",
    "        'action_reward': -0.3,\n",
    "        'slower_than_others_penalty': True # if True then reward is negative for speeds lower than 25m/s\n",
    "    },\n",
    "    policy_frequency={'policy_frequency': 8},\n",
    ")\n",
    "\n",
    "continuous_kinematics_2 = deepcopy(continuous_kinematics)\n",
    "\n",
    "continuous_kinematics_2['collision_reward'], continuous_kinematics_2['reward_speed_range'], continuous_kinematics_2['action_reward'] = -10, [15, 35], -0.01"
   ],
   "id": "59aae4b6091e41fd",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 1",
   "id": "f8798d3471b84d4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (training)",
   "id": "3b6fbf311610a62b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.687111Z",
     "start_time": "2024-06-11T13:21:16.682900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# vec_env_ck = make_vec_env(\n",
    "#     env_id='CustomHighwayEnv-v0',\n",
    "#     n_envs=5, # Number of parallel environments ran on different cpu_cores\n",
    "#     seed=0,\n",
    "#     vec_env_cls=SubprocVecEnv,\n",
    "#     env_kwargs={'config':continuous_kinematics, 'render_mode':None}\n",
    "# )"
   ],
   "id": "39594b3911753e33",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.692127Z",
     "start_time": "2024-06-11T13:21:16.688108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Policy network architecture\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch=[256, 256]  # network architecture\n",
    "# )\n",
    "# \n",
    "# # Create the PPO model\n",
    "# model = PPO(\n",
    "#     'MlpPolicy',\n",
    "#     vec_env_ck,\n",
    "#     n_steps=2048,  # Increased to gather more experience per update\n",
    "#     batch_size=256,  # Increased batch size\n",
    "#     learning_rate=3e-4,  # Adjusted learning rate for stability\n",
    "#     ent_coef=0.01,  # Higher entropy coefficient to encourage exploration\n",
    "#     clip_range=0.2,  # Clipping range for PPO\n",
    "#     vf_coef=0.5,  # Value function coefficient\n",
    "#     max_grad_norm=0.5,  # Gradient clipping\n",
    "#     policy_kwargs=policy_kwargs,\n",
    "#     verbose=2\n",
    "# )\n",
    "# \n",
    "# # Train the model\n",
    "# model.learn(total_timesteps=100_000)  # Number of timesteps for training\n",
    "# model.save('models/Continuous_Kinematics/ppo_1')"
   ],
   "id": "11714dd917d590e6",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (evalutation)",
   "id": "3d5b1a4ecb89162"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.765099Z",
     "start_time": "2024-06-11T13:21:16.693629Z"
    }
   },
   "cell_type": "code",
   "source": "ppo_1_ck = PPO.load('models/Continuous_Kinematics/ppo_1')",
   "id": "37d599036ef2b7c0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:16.806465Z",
     "start_time": "2024-06-11T13:21:16.766614Z"
    }
   },
   "cell_type": "code",
   "source": "env_ck_1 = create_env(id='CustomHighwayEnv-v0', render_mode='human', config=continuous_kinematics)",
   "id": "1cbe85e489977cce",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 7)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:33.399683Z",
     "start_time": "2024-06-11T13:21:16.810480Z"
    }
   },
   "cell_type": "code",
   "source": "results_ck_1 = evaluate_model(ppo_1_ck, env_ck_1, 'PPO_1')",
   "id": "1608b8fd3b93b92b",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:21:33.409557Z",
     "start_time": "2024-06-11T13:21:33.399683Z"
    }
   },
   "cell_type": "code",
   "source": "results_ck_1",
   "id": "6777ae211cd77b1f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                  returns_list\n",
       "0                     0.111854          0.433159  (0.11185377399936493, False)\n",
       "1                     2.434820          1.388849   (2.4348201719394678, False)\n",
       "2                    10.328912          3.400883   (10.328912310549395, False)\n",
       "3                     4.121161          1.809552    (4.121160747500981, False)\n",
       "4                     4.570697          1.953107    (4.570696677965851, False)\n",
       "5                     1.762089          1.196512   (1.7620888337065468, False)\n",
       "6                     4.893903          1.951734    (4.893903273889418, False)\n",
       "7                     2.275612          1.209923    (2.275611652012633, False)\n",
       "8                     0.059783          0.362678  (0.05978250963685562, False)\n",
       "9                     7.972792          2.598400    (7.972792316714172, False)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111854</td>\n",
       "      <td>0.433159</td>\n",
       "      <td>(0.11185377399936493, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.434820</td>\n",
       "      <td>1.388849</td>\n",
       "      <td>(2.4348201719394678, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.328912</td>\n",
       "      <td>3.400883</td>\n",
       "      <td>(10.328912310549395, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.121161</td>\n",
       "      <td>1.809552</td>\n",
       "      <td>(4.121160747500981, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.570697</td>\n",
       "      <td>1.953107</td>\n",
       "      <td>(4.570696677965851, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.762089</td>\n",
       "      <td>1.196512</td>\n",
       "      <td>(1.7620888337065468, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.893903</td>\n",
       "      <td>1.951734</td>\n",
       "      <td>(4.893903273889418, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.275612</td>\n",
       "      <td>1.209923</td>\n",
       "      <td>(2.275611652012633, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.059783</td>\n",
       "      <td>0.362678</td>\n",
       "      <td>(0.05978250963685562, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7.972792</td>\n",
       "      <td>2.598400</td>\n",
       "      <td>(7.972792316714172, False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 2",
   "id": "b4f1e9eda8aa9120"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (training)",
   "id": "690cfbe905c1704"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# vec_env_ck = make_vec_env(\n",
    "#     env_id='CustomHighwayEnv-v0',\n",
    "#     n_envs=5, # Number of parallel environments ran on different cpu_cores\n",
    "#     seed=0,\n",
    "#     vec_env_cls=SubprocVecEnv,\n",
    "#     env_kwargs={'config':continuous_kinematics_2, 'render_mode':None}\n",
    "# )"
   ],
   "id": "c4715568f89d1954",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Policy network architecture\n",
    "# policy_kwargs = dict(\n",
    "#     net_arch=[256, 256]  # network architecture\n",
    "# )\n",
    "# \n",
    "# # Create the PPO model\n",
    "# model = PPO(\n",
    "#     'MlpPolicy',\n",
    "#     vec_env_ck,\n",
    "#     n_steps=2048,  # Increased to gather more experience per update\n",
    "#     batch_size=256,  # Increased batch size\n",
    "#     learning_rate=3e-4,  # Adjusted learning rate for stability\n",
    "#     ent_coef=0.01,  # Higher entropy coefficient to encourage exploration\n",
    "#     clip_range=0.2,  # Clipping range for PPO\n",
    "#     vf_coef=0.5,  # Value function coefficient\n",
    "#     max_grad_norm=0.5,  # Gradient clipping\n",
    "#     policy_kwargs=policy_kwargs,\n",
    "#     verbose=2\n",
    "# )\n",
    "# \n",
    "# # Train the model\n",
    "# model.learn(total_timesteps=1_300_000)  # Number of timesteps for training\n",
    "# model.save('models/Continuous_Kinematics/ppo_2')"
   ],
   "id": "e11f0110df77d3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (evaluation)",
   "id": "bf4f8233808313e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:04:57.654996Z",
     "start_time": "2024-06-11T13:04:55.956983Z"
    }
   },
   "cell_type": "code",
   "source": "ppo_2_ck = PPO.load('models/Continuous_Kinematics/ppo_2')",
   "id": "8b5359d28b1a7db6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object clip_range. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:05:01.749702Z",
     "start_time": "2024-06-11T13:05:00.973909Z"
    }
   },
   "cell_type": "code",
   "source": "env_ck_1 = create_env(id='highway-v0', render_mode='human', config=continuous_kinematics_2)",
   "id": "7e9b27c36a1608c7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:42: UserWarning: \u001B[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (5, 7)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:09:16.210096Z",
     "start_time": "2024-06-11T13:07:05.892787Z"
    }
   },
   "cell_type": "code",
   "source": "results_ck_2 = evaluate_model(ppo_2_ck, env_ck_1, 'PPO_2')",
   "id": "4f148f936f9a3b76",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:11:16.236502Z",
     "start_time": "2024-06-11T13:11:16.222953Z"
    }
   },
   "cell_type": "code",
   "source": "results_ck_2",
   "id": "3696cb1a516bb3a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                 returns_list\n",
       "0                    55.107678         17.087796   (55.10767805471098, False)\n",
       "1                    21.729253          6.493152   (21.72925278878278, False)\n",
       "2                   119.274493         35.654491    (119.2744932674568, True)\n",
       "3                    27.504507          8.316161  (27.504506771033224, False)\n",
       "4                    11.418005          3.287526   (11.41800485024134, False)\n",
       "5                    31.784420          9.569394  (31.784419970317092, False)\n",
       "6                    15.463158          4.575972  (15.463157795141505, False)\n",
       "7                   119.074944         35.527438   (119.07494448395634, True)\n",
       "8                     9.549812          2.721412   (9.549812300893958, False)\n",
       "9                    22.261771          6.573251  (22.261770658069622, False)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55.107678</td>\n",
       "      <td>17.087796</td>\n",
       "      <td>(55.10767805471098, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.729253</td>\n",
       "      <td>6.493152</td>\n",
       "      <td>(21.72925278878278, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>119.274493</td>\n",
       "      <td>35.654491</td>\n",
       "      <td>(119.2744932674568, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27.504507</td>\n",
       "      <td>8.316161</td>\n",
       "      <td>(27.504506771033224, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.418005</td>\n",
       "      <td>3.287526</td>\n",
       "      <td>(11.41800485024134, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>31.784420</td>\n",
       "      <td>9.569394</td>\n",
       "      <td>(31.784419970317092, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15.463158</td>\n",
       "      <td>4.575972</td>\n",
       "      <td>(15.463157795141505, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>119.074944</td>\n",
       "      <td>35.527438</td>\n",
       "      <td>(119.07494448395634, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.549812</td>\n",
       "      <td>2.721412</td>\n",
       "      <td>(9.549812300893958, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>22.261771</td>\n",
       "      <td>6.573251</td>\n",
       "      <td>(22.261770658069622, False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "7fc84f81",
   "metadata": {},
   "source": [
    "## Solution 3 - DiscreteMeta_Grayscale\n",
    "Environment Observation Type: **Gray Scale** \\\n",
    "Agent Action Type: **DiscreteMetaAction** \\\n",
    "Algorithm Used: **DQN**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Configs",
   "id": "860d558c351a998"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:22:39.321773Z",
     "start_time": "2024-06-11T13:22:39.318761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "discrete_meta_grayscale_1 = config_generator(obs_action_type={\n",
    "        'observation': {\n",
    "            'type': 'GrayscaleObservation',\n",
    "            'observation_shape': (128, 64),\n",
    "            'stack_size': 4,\n",
    "            'weights': [0.2989, 0.5870, 0.1140],\n",
    "            'scaling': 1.75,\n",
    "        },\n",
    "        'action': {\n",
    "            'type': 'DiscreteMetaAction',\n",
    "        },\n",
    "    })"
   ],
   "id": "4c86260f1da33b04",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:22:39.484891Z",
     "start_time": "2024-06-11T13:22:39.480930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "discrete_meta_grayscale_2 = deepcopy(discrete_meta_grayscale_1)\n",
    "discrete_meta_grayscale_2['collision_reward'], discrete_meta_grayscale_2['reward_speed_range'] = -5, [25, 35]"
   ],
   "id": "d074ebebbc9b6c8e",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 1",
   "id": "c8022b47628f0231"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (training)",
   "id": "33ed11fade459898"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:22:39.490924Z",
     "start_time": "2024-06-11T13:22:39.486891Z"
    }
   },
   "cell_type": "code",
   "source": "# env_dmg = create_env(id='highway-v0',render_mode=None, config=discrete_meta_grayscale_2)",
   "id": "70bd70cbabe0b78b",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:22:39.521582Z",
     "start_time": "2024-06-11T13:22:39.518035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = DQN('CnnPolicy',\n",
    "#             env_dmg,\n",
    "#             learning_rate=5e-4,\n",
    "#             buffer_size=15000,\n",
    "#             learning_starts=200,\n",
    "#             batch_size=32,\n",
    "#             gamma=0.8,\n",
    "#             train_freq=1,\n",
    "#             gradient_steps=1,\n",
    "#             target_update_interval=50,\n",
    "#             exploration_fraction=0.7,\n",
    "#             verbose=1,\n",
    "#             tensorboard_log='highway_dqn/')\n",
    "# \n",
    "# model.learn(total_timesteps=int(2e4))\n",
    "# \n",
    "# model.save('models/DiscreteMeta_Grayscale/dqn_1')"
   ],
   "id": "a6e0b8f8d615e701",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (evaluation)",
   "id": "e2d91fdf34d437a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:22:39.749893Z",
     "start_time": "2024-06-11T13:22:39.583124Z"
    }
   },
   "cell_type": "code",
   "source": "dqn_1_dmg = DQN.load('models/DiscreteMeta_Grayscale/dqn_1')",
   "id": "30f5882c57b7f4e6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:22:39.778461Z",
     "start_time": "2024-06-11T13:22:39.752403Z"
    }
   },
   "cell_type": "code",
   "source": "env_dmg_1 = create_env(id='highway-v0',render_mode='human', config=discrete_meta_grayscale_1)",
   "id": "1710a37670e784e9",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:23:44.940550Z",
     "start_time": "2024-06-11T13:22:39.779964Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmg_1 = evaluate_model(dqn_1_dmg, env_dmg_1, 'DQN_1')",
   "id": "96870aef103c2ce3",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:23:44.950067Z",
     "start_time": "2024-06-11T13:23:44.942058Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmg_1",
   "id": "b108cd4678328f90",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                 returns_list\n",
       "0                     7.555966          3.676856   (7.555965514333092, False)\n",
       "1                    23.741156          8.519618  (23.741156460285943, False)\n",
       "2                     2.765820          1.336143  (2.7658195967702057, False)\n",
       "3                     1.805349          0.986621  (1.8053493042319608, False)\n",
       "4                   103.998501         39.546077   (103.99850105462943, True)\n",
       "5                     0.879994          0.684051  (0.8799940278395417, False)\n",
       "6                     7.683530          3.063447   (7.683530426172107, False)\n",
       "7                     1.962696          0.974907  (1.9626959530521901, False)\n",
       "8                    13.284627          5.155580  (13.284627096947291, False)\n",
       "9                     1.807965          0.977983  (1.8079650703103967, False)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.555966</td>\n",
       "      <td>3.676856</td>\n",
       "      <td>(7.555965514333092, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23.741156</td>\n",
       "      <td>8.519618</td>\n",
       "      <td>(23.741156460285943, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.765820</td>\n",
       "      <td>1.336143</td>\n",
       "      <td>(2.7658195967702057, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.805349</td>\n",
       "      <td>0.986621</td>\n",
       "      <td>(1.8053493042319608, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103.998501</td>\n",
       "      <td>39.546077</td>\n",
       "      <td>(103.99850105462943, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.879994</td>\n",
       "      <td>0.684051</td>\n",
       "      <td>(0.8799940278395417, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.683530</td>\n",
       "      <td>3.063447</td>\n",
       "      <td>(7.683530426172107, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.962696</td>\n",
       "      <td>0.974907</td>\n",
       "      <td>(1.9626959530521901, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.284627</td>\n",
       "      <td>5.155580</td>\n",
       "      <td>(13.284627096947291, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.807965</td>\n",
       "      <td>0.977983</td>\n",
       "      <td>(1.8079650703103967, False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model 2",
   "id": "b26840b1551031e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (training)",
   "id": "a1274c4390d2f3b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:23:44.955574Z",
     "start_time": "2024-06-11T13:23:44.951573Z"
    }
   },
   "cell_type": "code",
   "source": "# env_dmg = create_env(id='highway-v0',render_mode=None, config=discrete_meta_grayscale_1)",
   "id": "50dea201bcaa8a0a",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:23:44.960769Z",
     "start_time": "2024-06-11T13:23:44.957245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model_2 = DQN(\n",
    "#     'CnnPolicy',\n",
    "#     env_dmg,\n",
    "#     learning_rate=3e-4,             # Slightly reduced learning rate for stability\n",
    "#     buffer_size=50000,              # Increased buffer size for more experience\n",
    "#     learning_starts=1000,           # More initial exploration before learning starts\n",
    "#     batch_size=32,                  # Keeping batch size the same\n",
    "#     gamma=0.9,                      # Increased gamma to consider long-term rewards\n",
    "#     train_freq=4,                   # Training less frequently\n",
    "#     gradient_steps=4,               # More gradient steps to balance less frequent training\n",
    "#     target_update_interval=100,     # Update target network less frequently\n",
    "#     exploration_fraction=0.5,       # Reduced exploration fraction for earlier exploitation\n",
    "#     verbose=1,\n",
    "# )\n",
    "# \n",
    "# model_2.learn(total_timesteps=int(2e5))\n",
    "# \n",
    "# model_2.save('models/DiscreteMeta_Grayscale/dqn_2')"
   ],
   "id": "7a41a1f3d4f21249",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### (evaluation)",
   "id": "957c2bc31d582072"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:23:45.072641Z",
     "start_time": "2024-06-11T13:23:44.961772Z"
    }
   },
   "cell_type": "code",
   "source": "dqn_2_dmg = DQN.load('models/DiscreteMeta_Grayscale/dqn_2')",
   "id": "3e0e0d30c088a74c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object lr_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n",
      "C:\\Users\\pedro\\anaconda3\\envs\\py310\\lib\\site-packages\\stable_baselines3\\common\\save_util.py:167: UserWarning: Could not deserialize object exploration_schedule. Consider using `custom_objects` argument to replace this object.\n",
      "Exception: code expected at most 16 arguments, got 18\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:23:45.096974Z",
     "start_time": "2024-06-11T13:23:45.073641Z"
    }
   },
   "cell_type": "code",
   "source": "env_dmg_1 = create_env(id='highway-v0',render_mode='human', config=discrete_meta_grayscale_1)",
   "id": "5df25cc06d1f27e",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.119630Z",
     "start_time": "2024-06-11T13:23:45.097978Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmg_4 = evaluate_model(dqn_2_dmg, env_dmg_1, 'DQN_2')",
   "id": "181997f28f507f7a",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.129655Z",
     "start_time": "2024-06-11T13:25:36.120639Z"
    }
   },
   "cell_type": "code",
   "source": "results_dmg_4",
   "id": "5b8f650a446e07f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                 returns_list\n",
       "0                    14.738975          5.848241  (14.738975149431306, False)\n",
       "1                     7.277482          3.080122      (7.277482409018, False)\n",
       "2                    32.864879         12.902146   (32.86487936740899, False)\n",
       "3                    50.224053         17.038968   (50.22405265471093, False)\n",
       "4                    11.196617          4.329761  (11.196617130545592, False)\n",
       "5                    22.977891          8.762615   (22.97789075172317, False)\n",
       "6                    41.159169         15.265447   (41.15916941319433, False)\n",
       "7                    88.902291         38.787627    (88.90229110834827, True)\n",
       "8                     9.134590          3.793373     (9.1345898172851, False)\n",
       "9                     1.686163          0.958965  (1.6861629424681035, False)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.738975</td>\n",
       "      <td>5.848241</td>\n",
       "      <td>(14.738975149431306, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.277482</td>\n",
       "      <td>3.080122</td>\n",
       "      <td>(7.277482409018, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.864879</td>\n",
       "      <td>12.902146</td>\n",
       "      <td>(32.86487936740899, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.224053</td>\n",
       "      <td>17.038968</td>\n",
       "      <td>(50.22405265471093, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.196617</td>\n",
       "      <td>4.329761</td>\n",
       "      <td>(11.196617130545592, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.977891</td>\n",
       "      <td>8.762615</td>\n",
       "      <td>(22.97789075172317, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>41.159169</td>\n",
       "      <td>15.265447</td>\n",
       "      <td>(41.15916941319433, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88.902291</td>\n",
       "      <td>38.787627</td>\n",
       "      <td>(88.90229110834827, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.134590</td>\n",
       "      <td>3.793373</td>\n",
       "      <td>(9.1345898172851, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.686163</td>\n",
       "      <td>0.958965</td>\n",
       "      <td>(1.6861629424681035, False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "id": "d66f0daf",
   "metadata": {},
   "source": [
    "## Solution 4 - Continuous_Grayscale\n",
    "Environment Observation Type: **Gray Scale** \\\n",
    "Agent Action Type: **Continuous Actions** \\\n",
    "Algorithm Used: **SAC + PPO**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Config",
   "id": "826e49b00398f154"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.136302Z",
     "start_time": "2024-06-11T13:25:36.130660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "continuous_grayscale = config_generator(\n",
    "    obs_action_type={\n",
    "        'observation': {\n",
    "           'type': 'GrayscaleObservation',\n",
    "           'observation_shape': (128, 64),\n",
    "           'stack_size': 8,\n",
    "           'weights': [0.2989, 0.5870, 0.1140],\n",
    "           'scaling': 1,\n",
    "        },\n",
    "        'action': {\n",
    "            'type': 'ContinuousAction',\n",
    "            'longitudinal': True,\n",
    "            'lateral': True,\n",
    "            'acceleration_range': [-1, 1],\n",
    "            'steering_range': [-0.15, 0.15],\n",
    "            'speed_range': [0, 40]\n",
    "        }\n",
    "    }, \n",
    "    reward_params={\n",
    "        'collision_reward': -1, #\n",
    "        'reward_speed_range': [0, 40], #\n",
    "        # 'high_speed_reward': 2, # this is renamed for the sake of interpretability\n",
    "        'speed_reward': 2, # high_speed_reward renamed\n",
    "        'stationary_penalty': -1, # amount of reward fed into the agent if stationary for too many steps\n",
    "        'lane_change_penalty': -4, #\n",
    "        'offroad_penalty': -1, #\n",
    "        'lane_centering_cost': 4, #\n",
    "        'lane_centering_reward': 1, #\n",
    "        'slower_than_others_penalty': False, # if True then reward is negative for speeds lower than 25m/s\n",
    "        'simulation_frequency': 15,\n",
    "        'offroad_terminal': True,\n",
    "        'normalize_reward': True,\n",
    "        'dynamical': False,\n",
    "    },\n",
    "    policy_frequency={'policy_frequency': 4}\n",
    ")"
   ],
   "id": "16217147e106c0f7",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.141750Z",
     "start_time": "2024-06-11T13:25:36.137881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "continuous_grayscale_pf8 = deepcopy(continuous_grayscale)\n",
    "continuous_grayscale_pf8['policy_frequency'] = 8"
   ],
   "id": "1dec988276c9ac3c",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Model SAC",
   "id": "f3cf9f6960c6005e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### (training)",
   "id": "400d98a61573dd87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.146920Z",
     "start_time": "2024-06-11T13:25:36.142779Z"
    }
   },
   "cell_type": "code",
   "source": "# vec_env_cg = make_vec_env(env_id='CustomHighwayEnv-v0', n_envs=5, vec_env_cls=DummyVecEnv, env_kwargs={'config':continuous_grayscale})",
   "id": "45c8c57e9d71078d",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.152073Z",
     "start_time": "2024-06-11T13:25:36.147924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model = SAC(\n",
    "#     'CnnPolicy',\n",
    "#     vec_env_cg,\n",
    "#     verbose=1,\n",
    "#     buffer_size=15000,\n",
    "#     batch_size=64,\n",
    "#     learning_rate=0.0001,\n",
    "#     gamma=0.92,\n",
    "# )\n",
    "# \n",
    "# model.learn(total_timesteps=1000000, log_interval=4)\n",
    "# \n",
    "# model.save('models/Continuous_Grayscale/sac_2_continuous_grayscale')"
   ],
   "id": "3bc745ce4e839046",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###### (evaluation)",
   "id": "22e630e6e01693d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.510844Z",
     "start_time": "2024-06-11T13:25:36.152591Z"
    }
   },
   "cell_type": "code",
   "source": "sac_cg = SAC.load('models/Continuous_Grayscale/sac.zip')",
   "id": "8f59df8b4bf39295",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:25:36.537449Z",
     "start_time": "2024-06-11T13:25:36.511900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# training with lower policy_frequency and evaluating with higher seems to lead to better results in a continuous environment\n",
    "env_cg_1 = create_env(id='highway-v0',render_mode='human', config=continuous_grayscale_pf8)\n",
    "# env_cg_2 = create_env(id='CustomHighwayEnv-v0', render_mode='human', config=continuous_grayscale_pf8)"
   ],
   "id": "80c01f3298b96aab",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:27:59.310766Z",
     "start_time": "2024-06-11T13:25:36.538452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# (~>110, True) = Completed | (~<100, True) = Stayed behind | (x, False) = crashed or went offroad\n",
    "results_cg_1_df = evaluate_model(sac_cg, env_cg_1, 'SAC')"
   ],
   "id": "7f15a04afc0f82bd",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:27:59.320750Z",
     "start_time": "2024-06-11T13:27:59.312699Z"
    }
   },
   "cell_type": "code",
   "source": "results_cg_1_df",
   "id": "c7a76708bfdb6afb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   average_standardized_reward  episode_duration                 returns_list\n",
       "0                    91.086962         29.622779    (91.08696207775449, True)\n",
       "1                    20.276030          5.314952   (20.27602987961856, False)\n",
       "2                    39.537728         10.634967   (39.53772834457063, False)\n",
       "3                    53.359172         14.501132   (53.35917241684028, False)\n",
       "4                    19.387417          5.388121  (19.387416637691064, False)\n",
       "5                   116.842809         29.583820   (116.84280911384374, True)\n",
       "6                    51.647301         13.642620   (51.64730135207125, False)\n",
       "7                    65.053748         17.475284    (65.0537479430315, False)\n",
       "8                    30.823429          8.365913   (30.82342941598586, False)\n",
       "9                    27.225796          7.693783  (27.225796392766455, False)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_standardized_reward</th>\n",
       "      <th>episode_duration</th>\n",
       "      <th>returns_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>91.086962</td>\n",
       "      <td>29.622779</td>\n",
       "      <td>(91.08696207775449, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.276030</td>\n",
       "      <td>5.314952</td>\n",
       "      <td>(20.27602987961856, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.537728</td>\n",
       "      <td>10.634967</td>\n",
       "      <td>(39.53772834457063, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53.359172</td>\n",
       "      <td>14.501132</td>\n",
       "      <td>(53.35917241684028, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19.387417</td>\n",
       "      <td>5.388121</td>\n",
       "      <td>(19.387416637691064, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>116.842809</td>\n",
       "      <td>29.583820</td>\n",
       "      <td>(116.84280911384374, True)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51.647301</td>\n",
       "      <td>13.642620</td>\n",
       "      <td>(51.64730135207125, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>65.053748</td>\n",
       "      <td>17.475284</td>\n",
       "      <td>(65.0537479430315, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>30.823429</td>\n",
       "      <td>8.365913</td>\n",
       "      <td>(30.82342941598586, False)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>27.225796</td>\n",
       "      <td>7.693783</td>\n",
       "      <td>(27.225796392766455, False)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:27:59.325777Z",
     "start_time": "2024-06-11T13:27:59.322267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# (~>220, True) = Completed | (~140, True) = Stayed behind | (x, False) = crashed or went offroad\n",
    "# results_cg_2_df = evaluate_model(sac_cg, env_cg_2, 'SAC')"
   ],
   "id": "f1f40a0059aa7d74",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:27:59.332379Z",
     "start_time": "2024-06-11T13:27:59.326777Z"
    }
   },
   "cell_type": "code",
   "source": "# results_cg_2_df",
   "id": "b3fd5fe58ffdab9",
   "outputs": [],
   "execution_count": 73
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nova",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
